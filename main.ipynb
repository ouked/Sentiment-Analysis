{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis Coursework\n",
    "Alex Dawkins (asd60@bath.ac.uk), python 3.8\n",
    "\n",
    "The aim of this coursework is to write a sentiment analysis application to classify\n",
    "movie reviews as either **positive**, or **negative**.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "The dataset is 25,000 highly polar movie reviews. It has already been split into training and testing subsets.\n",
    "The dataset can be found [here](https://ai.stanford.edu/~amaas/data/sentiment/).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexdawkins/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexdawkins/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from typing import List\n",
    "\n",
    "# Import and setup NLTK\n",
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.lm import Vocabulary\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding reviews...\n",
      "Loading reviews...\n",
      "Extracting ratings...\n",
      "Loaded 1000 positive reviews, and 1000 negative reviews.\n",
      "---\n",
      "pos_reviews[0]='For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.'\n",
      "neg_reviews[0]=\"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\"\n"
     ]
    }
   ],
   "source": [
    "# Load the reviews\n",
    "import os\n",
    "n_reviews = 25_000\n",
    "data_dir = '../../data'\n",
    "train_dir = f'{data_dir}/aclImdb/train'\n",
    "test_dir = f'{data_dir}/aclImdb/test'\n",
    "neg_train_dir = f'{train_dir}/neg'\n",
    "pos_train_dir = f'{train_dir}/pos'\n",
    "\n",
    "pos_test_dir = f'{test_dir}/pos'\n",
    "neg_test_dir = f'{test_dir}/neg'\n",
    "\n",
    "\n",
    "def load_reviews(fps: List[str], dir_: str, max_n: int = -1) -> List[str]:\n",
    "    fps_cut = fps\n",
    "    if max_n != -1:\n",
    "        fps_cut = fps[:max_n]\n",
    "    reviews = []\n",
    "    for fp in fps_cut:\n",
    "        with open(dir_ + '/' + fp, 'r') as f:\n",
    "            reviews.append(f.read())\n",
    "\n",
    "    return reviews\n",
    "\n",
    "def get_rating(fp: str) -> int:\n",
    "    try:\n",
    "        return int(fp.split('_')[1].split('.')[0])\n",
    "    except (ValueError, IndexError) as e:\n",
    "        raise Exception(f\"Couldn't extract rating from filepath: '{fp}'\") from e\n",
    "\n",
    "\n",
    "print(\"Finding reviews...\")\n",
    "neg_fps = [fp for fp in os.listdir(neg_train_dir) if fp.endswith('.txt')]\n",
    "pos_fps = [fp for fp in os.listdir(pos_train_dir) if fp.endswith('.txt')]\n",
    "test_pos_fps = [fp for fp in os.listdir(pos_test_dir) if fp.endswith('.txt')]\n",
    "test_neg_fps = [fp for fp in os.listdir(neg_test_dir) if fp.endswith('.txt')]\n",
    "\n",
    "print(\"Loading reviews...\")\n",
    "pos_reviews = load_reviews(pos_fps, pos_train_dir, 1000)\n",
    "neg_reviews = load_reviews(neg_fps, neg_train_dir, 1000)\n",
    "test_pos_reviews = load_reviews(test_pos_fps, pos_test_dir, 100)\n",
    "test_neg_reviews = load_reviews(test_neg_fps, neg_test_dir, 100)\n",
    "# print(f\"{test_pos_reviews[0]=}\")\n",
    "\n",
    "print(\"Extracting ratings...\")\n",
    "pos_ratings = [get_rating(fp) for fp in pos_fps][:len(pos_reviews)]\n",
    "neg_ratings = [get_rating(fp) for fp in neg_fps][:len(neg_reviews)]\n",
    "test_pos_ratings = [get_rating(fp) for fp in test_pos_fps][:len(test_pos_reviews)]\n",
    "test_neg_ratings = [get_rating(fp) for fp in test_neg_fps][:len(test_neg_reviews)]\n",
    "\n",
    "print(f\"Loaded {len(pos_reviews)} positive reviews, and {len(neg_reviews)} negative reviews.\\n---\")\n",
    "print(f\"{pos_reviews[0]=}\")\n",
    "print(f\"{neg_reviews[0]=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although the reviews in the dataset are written by many different people, it's possible that there could be some\n",
    "deviations in the lengths of reviewers' words or sentences, depending on whether they are talking about something\n",
    "in a positive or a negative manner.\n",
    "\n",
    "Perhaps more critical reviews are more likely to use longer, more technical words as the reviewers wants to use technical\n",
    "language to reason their point.\n",
    "\n",
    "Alternatively, there might be no correlation at all, as any consistency between up to 12,500 reviews is quite unlikely.\n",
    "This approach is more likely to be effective to distinguish between two different authors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 28 14 pos\n",
      "5 26 14 neg\n"
     ]
    }
   ],
   "source": [
    "def lengths(data: str, name: str):\n",
    "    words = nltk.tokenize.word_tokenize(data)\n",
    "    num_words = len(words)\n",
    "    avg_word_len = round(len(data) / num_words)\n",
    "    avg_sent_len = round(num_words / len(nltk.tokenize.sent_tokenize(data)))\n",
    "    # average number of times each word occurs uniquely\n",
    "    avg_n_unique_word = round(num_words / len(set(w.lower() for w in words)))\n",
    "    print(avg_word_len, avg_sent_len, avg_n_unique_word, name)\n",
    "\n",
    "# Turn the lists into strings\n",
    "all_pos_reviews = '\\n'.join(pos_reviews)\n",
    "all_neg_reviews = '\\n'.join(neg_reviews)\n",
    "\n",
    "lengths(all_pos_reviews, 'pos')\n",
    "lengths(all_neg_reviews, 'neg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There doesn't appear to be any difference at all! **This technique will not work for categorising the data,** as both\n",
    "positive and negative reviews have very similar word and sentence length, and amount of unique vocabulary.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As part of the data set, a tokenised list of words (`imdb.vocab`), and the associated expected rating for each token\n",
    "(`imdbEr.txt`). This list of expected ratings was computed by (Potts, 2011).\n",
    "\n",
    "We can take the sum of each word's expected rating as the review's expected rating."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "def try_make_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return 0.\n",
    "\n",
    "with open(f'{data_dir}/aclImdb/imdb.vocab', 'r') as f:\n",
    "        vocab = f.read().split('\\n')\n",
    "\n",
    "# faster than vocab.index()\n",
    "vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "with open(f'{data_dir}/aclImdb/imdbEr.txt', 'r') as f:\n",
    "    expected_ratings = list(map(try_make_float, f.read().split('\\n')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def evaluate(to_test, *, positive, modifier=None):\n",
    "    if not modifier:\n",
    "        def modifier(_):\n",
    "            return 1\n",
    "    n_positive = 0\n",
    "    for i, rev in enumerate(to_test):\n",
    "        words = [word.lower() for word in nltk.word_tokenize(rev) if word not in '.,\\'\"']\n",
    "        expected_rating = 0\n",
    "        n_words = len(words)\n",
    "        for j, word in enumerate(words):\n",
    "            idx = vocab_index.get(word, None)\n",
    "            if not idx:\n",
    "                continue\n",
    "            expected_rating += modifier(j/n_words) * expected_ratings[idx]\n",
    "\n",
    "        if (expected_rating > 0) == positive:\n",
    "            # print(n_positive, i)\n",
    "            n_positive += 1\n",
    "\n",
    "    print(f\"Accuracy: {n_positive}/{len(to_test)} ({n_positive/len(to_test)})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79/100 (0.79)\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_pos_reviews, positive=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75/100 (0.75)\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_neg_reviews, positive=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From reading some of the reviews, I found that a lot of them include a lot of pre-amble, describing the context of how\n",
    "the reviewer watched the movie, their initial thoughts, etc. These words are independent of the sentiment of the whole\n",
    "review.\n",
    "\n",
    "For example, `pos/13_9.txt`:\n",
    "\n",
    " >I work at a movie theater and every Thursday night we have an employee screening of one movie that comes out the next day...Today it was The Guardian. I saw the trailers and the ads and never expected much from it, and in no way really did i anticipate seeing this...\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def parabola(x_):\n",
    "    return (-(1.75 * x_ - 0.875) ** 2) + 1\n",
    "def positive_gradient(x_):\n",
    "    return ( 0.5 * x_ ) + 0.5\n",
    "def negative_gradient(x_):\n",
    "    return -( 0.5 * x_ ) + 1\n",
    "\n",
    "\n",
    "x = np.linspace(0, 1, 41)\n",
    "plt.plot(x, parabola(x),label=\"Parabola\")\n",
    "plt.plot(x, positive_gradient(x),label=\"+ve Grad.\")\n",
    "plt.plot(x, negative_gradient(x),label=\"-ve Grad.\")\n",
    "plt.xlabel('Relative word position')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate(modifier=parabola)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate(modifier=positive_gradient)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some reviews get straight to the point, such as `pos/17_8.txt`:\n",
    " > Brilliant and moving performances by Tom Courtenay and Peter Finch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate(modifier=negative_gradient)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding weights for the positions of words within the reviews doesn't appear to significantly improve performance, and I\n",
    "haven't experimented enough to warrant keeping this technique within the solution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As using Potts' list appears to be very effective, I wanted to try to recreate the list, using the training data.\n",
    "\n",
    "This code iterates through each word in every review and assigns the word a portion of the rating: if a review is 10\n",
    "stars, and it contains 2 words, these 2 words are clearly (and assumed equally) positive words, and should be considered\n",
    "more significant than a word that appears once in a long review."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "def clean(x):\n",
    "    return regex.sub(' ', x).lower()\n",
    "\n",
    "def make_er(train_data, ratings, use_cache=False):\n",
    "    if use_cache:\n",
    "        with open('results.txt', 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    vocab_ratings = {}\n",
    "    vocab_occurrences = {}\n",
    "    porter = nltk.PorterStemmer()\n",
    "    n_reviews = len(train_data)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i, rev in enumerate(train_data):\n",
    "        words = [word for word in nltk.word_tokenize(clean(rev)) if word not in stop_words]\n",
    "        stems = [porter.stem(word) for word in words]\n",
    "        n_words = len(words)\n",
    "        # Convert (1 to 10) to (-5 to 5)\n",
    "        rating = ratings[i]\n",
    "        if rating > 5:\n",
    "            rating -= 5\n",
    "        else:\n",
    "            rating -= 6\n",
    "        rel_rating = rating / n_words\n",
    "        for word in stems:\n",
    "            vocab_ratings[word] = vocab_ratings.get(word, 0) + rel_rating\n",
    "            vocab_occurrences[word] = vocab_occurrences.get(word, 0) + 1\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"{i}/{n_reviews}...\")\n",
    "    vocab_ratings = {word: sum_ / vocab_occurrences[word] for word, sum_ in vocab_ratings.items()}\n",
    "    with open('results.txt', 'w') as f:\n",
    "        f.write(json.dumps(vocab_ratings))\n",
    "\n",
    "    return vocab_ratings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, this algorithm gives poor results, which are only marginally better than guessing randomly.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Generating ERs...\")\n",
    "# expected_ratings_dict = make_er(pos_reviews + neg_reviews, pos_ratings + neg_ratings)\n",
    "expected_ratings_dict = make_er([], [], use_cache=True)\n",
    "\n",
    "positive_reviews = True\n",
    "to_test = test_pos_reviews if positive_reviews else test_neg_reviews\n",
    "def evaluate():\n",
    "    porter = nltk.PorterStemmer()\n",
    "    n_correct = 0\n",
    "    n_reviews = len(to_test)\n",
    "    for i, rev in enumerate(to_test):\n",
    "        words = nltk.word_tokenize(clean(rev))\n",
    "        stems = [porter.stem(word) for word in words]\n",
    "        sum_expected_rating = 0\n",
    "        n_words = len(words)\n",
    "        for j, word in enumerate(stems):\n",
    "            expected_rating = expected_ratings_dict.get(word, 0)\n",
    "            sum_expected_rating += expected_rating\n",
    "\n",
    "        if (sum_expected_rating > 0) == positive_reviews:\n",
    "            n_correct += 1\n",
    "\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            print(f\"Processed: {i}/{n_reviews}...\")\n",
    "            print(f\"Accuracy: {n_correct}/{i} ({n_correct / i})...\\n\")\n",
    "\n",
    "    print(\"Done evaluating.\")\n",
    "    print(f\"Accuracy: {n_correct}/{len(to_test)} ({n_correct / len(to_test)})\")\n",
    "\n",
    "    print(\"Evaluating...\")\n",
    "    evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, we must try another method.\n",
    "\n",
    "After researching popular sentiment analysis methods on the internet, I learnt about BoW and TF-IDF.\n",
    "\n",
    "## BoW\n",
    "Bag of Words (BoW) is a vectorisation technique that allows us to store a document in terms of the presence (or absence)\n",
    "of each word in the global vocabulary.\n",
    "\n",
    "If our global vocabulary is \"the weather is good bad\",\n",
    "we can store \"the weather is good\" as [1, 1, 1, 1, 0], and \"the weather is bad\" as [1, 1, 1, 0, 1].\n",
    "\n",
    "This makes it easier for Machine Learning models to work with the data, as the actual word isn't necessary.\n",
    "\n",
    "## TF-IDF\n",
    "TF-IDF means \"Term Frequency - Inverse Document Frequency\" and is a weight that signifies how important a word is in the\n",
    "corpus.\n",
    "\n",
    "TF is how frequently a term occurs in a given document.\n",
    "\n",
    "`TF(w) = number of occurences of w / number of words in document`\n",
    "\n",
    "IDF signifies how important a word is. Stop words like \"and\", \"is\" and \"the\" will have a very high\n",
    "term frequency, but they aren't significant in sentiment analysis. The IDF is low for common terms, and high\n",
    "for rare ones.\n",
    "\n",
    "`IDF(w) = log(Number of documents / Number of documents with w in)`\n",
    "\n",
    "SciKit Learn comes with a built-in TF-IDF vectoriser, but to fully understand the process, I implemented it myself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "sw = set(stopwords.words())\n",
    "html_tag = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "\n",
    "def make_tfidf(reviews, vocab):\n",
    "    # Build TF matrix\n",
    "    tf = np.ndarray(shape=(len(reviews), len(vocab)))\n",
    "    idf = []\n",
    "    for i, review in enumerate(reviews):\n",
    "        clean_review = re.sub(html_tag, '', review)\n",
    "        tokens = nltk.word_tokenize(clean_review)\n",
    "        stems = [ps.stem(token, to_lowercase=True) for token in tokens if token not in sw]\n",
    "\n",
    "        tf[i] = [stems.count(stem) for stem in sorted(vocab)]\n",
    "\n",
    "    n_docs = len(reviews)\n",
    "    # Calculate Inverse Document Frequencies\n",
    "    for i, word in enumerate(sorted(vocab)):\n",
    "        if word not in vocab or word == '<UNK>':\n",
    "            idf.append(0)\n",
    "            continue\n",
    "        docs_with_word = 0\n",
    "        for row in tf:\n",
    "            if row[i] > 0:\n",
    "                docs_with_word += 1\n",
    "\n",
    "        if docs_with_word == 0:\n",
    "            idf.append(0)\n",
    "            continue\n",
    "\n",
    "        x = n_docs / docs_with_word\n",
    "        assert x >= 1, word\n",
    "\n",
    "        idf.append(math.log(x))\n",
    "    return tf * idf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vocab...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "reviews = pos_reviews + neg_reviews\n",
    "test_reviews = test_pos_reviews + test_neg_reviews\n",
    "\n",
    "ratings = pos_ratings + neg_ratings\n",
    "test_ratings = test_pos_ratings + test_neg_ratings\n",
    "\n",
    "print(\"Collecting vocab...\")\n",
    "# https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
    "all_words = []\n",
    "for i, review in enumerate(reviews):\n",
    "    # Remove HTML tags\n",
    "    clean_review = re.sub(html_tag, '', review)\n",
    "    tokens = nltk.word_tokenize(clean_review)\n",
    "    stems = [ps.stem(token, to_lowercase=True) for token in tokens if token not in sw]\n",
    "\n",
    "    # remove all punctuation\n",
    "    stems = [x for x in [\"\".join(c for c in s if c not in string.punctuation) for s in stems] if x]\n",
    "    all_words += stems\n",
    "\n",
    "vocab = Vocabulary(all_words, unk_cutoff=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making TFIDF for training data...\n",
      "Making TFIDF for testing data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Making TFIDF for training data...\")\n",
    "tfidf = make_tfidf(reviews, vocab)\n",
    "print(\"Making TFIDF for testing data...\")\n",
    "tfidf_test = make_tfidf(test_reviews, vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then use Multinomial Naive Bayes (MNB) from scikit learn to create a model for our reviews.\n",
    "Gaussian Naive Bayes (GNB) assumes the vectors are continuous (such as temperature and time),\n",
    "but our vectors are integer counts, so we will use MNB, which is designed for counts or relative frequency."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "sentiment = [\"pos\" if rating > 5 else \"neg\" for rating in ratings]\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "model = mnb.fit(tfidf, sentiment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model...\n",
      "148 correct out of 200. (0.74)\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing model...\")\n",
    "results = mnb.predict(tfidf_test)\n",
    "\n",
    "n_correct = 0\n",
    "for i, rating in enumerate(test_ratings):\n",
    "    actual = \"pos\" if rating > 5 else \"neg\"\n",
    "    predict = results[i]\n",
    "    if actual == predict:\n",
    "        n_correct += 1\n",
    "\n",
    "print(f\"{n_correct} correct out of {len(results)}. ({n_correct/len(results)})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This implementation gives similar results to the Potts 2011 implementation!\n",
    "\n",
    "I believe my implementation of TFIDF is quite inefficient, so instead let's use the SciKit Learn vectoriser.\n",
    "\n",
    "the sklearn implementation doesn't clean the data as it goes like mine, so we will clean the reviews before vectorising them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def clean_reviews(reviews_):\n",
    "    clean_reviews = []\n",
    "    for review in reviews_:\n",
    "        clean_review = re.sub(html_tag, '', review)\n",
    "        tokens = nltk.word_tokenize(clean_review)\n",
    "        stems = [ps.stem(token, to_lowercase=True) for token in tokens if token not in sw]\n",
    "        clean_reviews.append(\" \".join(stems))\n",
    "    return clean_reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aditya' 'adjac' 'adjani' 'adjoin' 'adjunct' 'adjust' 'adjut' 'administ'\n",
      " 'administr' 'admir' 'admiss' 'admit' 'admitt' 'admittedli' 'adolesc'\n",
      " 'adolf' 'adopt' 'ador' 'adorn' 'adrenalin' 'adrian' 'adriana' 'adrienn'\n",
      " 'adul' 'adult' 'adulter' 'adulteri' 'adulthood' 'advanc' 'advani'\n",
      " 'advantag' 'advantage' 'advent' 'adventist' 'adventur' 'adventuresom'\n",
      " 'advers' 'adversari' 'advert' 'adverter' 'advertis' 'advertising' 'advic'\n",
      " 'advis' 'advoc' 'aeon' 'aerial' 'aerodynam' 'aeryn' 'aesthet']\n",
      "len(X_train)=2000, len(y_train)=2000\n",
      "len(X_test)=200, len(y_test)=200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# To access the global vocabulary, we produce tfidfs of training and testing together.\n",
    "all_tfidf = vectorizer.fit_transform(clean_reviews(reviews+test_reviews)).toarray()\n",
    "\n",
    "# We can use either method to split, but as we're comparing to my implementation from earlier, lets use the same data for train / test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(all_tfidf, (ratings+test_ratings), test_size=0.1, random_state=0)\n",
    "X_train, X_test = all_tfidf[:len(reviews)], all_tfidf[len(reviews):]\n",
    "y_train, y_test = ratings, test_ratings\n",
    "\n",
    "print(vectorizer.get_feature_names_out()[500:550])\n",
    "print(f\"{len(X_train)=}, {len(y_train)=}\")\n",
    "print(f\"{len(X_test)=}, {len(y_test)=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "model = mnb.fit(X_train, y_train)\n",
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model...\n",
      "140 correct out of 200. (0.7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing model...\")\n",
    "y_pred = mnb.predict(X_test)\n",
    "\n",
    "n_correct = 0\n",
    "for i, actual in enumerate(y_test):\n",
    "    predict = y_pred[i]\n",
    "    if (actual > 5) == (predict > 5):\n",
    "        n_correct += 1\n",
    "\n",
    "print(f\"{n_correct} correct out of {len(y_test)}. ({n_correct/len(y_test)})\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using SciKit Learn's Vectorizer gives comparable results to my own implementation (and is a lot faster at calculating the\n",
    "TF-IDFs!)\n",
    "\n",
    "## Conslusion\n",
    "\n",
    "In this report, we have explored the data set, and analysed it for possible features that could be effective\n",
    "in determining the sentiment. We have implemented a baseline model that uses other research from others, and experimented\n",
    "with a possible improvement, which didn't add any significant improvement. We then attempted to recreate the dataset\n",
    "that we used with a naive method, but couldn't produce any useful results.\n",
    "\n",
    "We then explored a further approach, using a different analysis method, TFIDF, and wrote an implementation that\n",
    "successfully classifies the reviews as positive or negative, the majority of the time. We then compared our\n",
    "implementation to another, de-facto implementation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}